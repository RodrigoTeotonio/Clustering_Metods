[
["index.html", "Análise de formação de agrupamentos pelos algoritmos não supervisionados K-Means, PAM e Clara 1 Motivação", " Análise de formação de agrupamentos pelos algoritmos não supervisionados K-Means, PAM e Clara Rodrigo Teotônio 1 Motivação Com o avanço dos recursos em software e hardware e dos usos das redes de computadores é crescente a geração e o armazenamento de dados como resultado das diversas atividades humanas. Vivemos em uma sociedade da informação, e se os dados são caracterizados como fatos registrados, a informação é o conjunto de padrões, ou expectativas que estão subjacentes aos dados. Há uma enorme quantidade de informações “escondidas” em bancos de dados - informações potencialmente importantes, mas ainda não foram descobertas ou articuladas. Assim, para que esses dados sejam úteis e gerem conhecimento é necessário fazer a “mineração e a análise dos dados”, que consiste em organizar dados, encontrar padrões, associações, mudanças e anomalias relevantes. Neste contexto, duas áreas tornam-se importantes: “Data Mining” ou minineração de dados, e Machine Learning ou aprendizado de máquinas. A mineração de dados é a extração de informações implícitas, anteriormente desconhecidas e potencialmente úteis a partir de dados. A idéia é criar programas de computador que examinem automaticamente bancos de dados, buscando regularidades ou padrões. Padrões permitem generalização para fazer previsões em dados futuros. Naturalmente, todo algoritmo tem sua falha e haverá problemas, e gerarão padrões corriqueiros e desinteressantes, outros serão hipotéticos e ilegíveis. Assim, é necessário algoritmos robustos o suficiente para lidar com dados imperfeitos e extrair regularidades que são inexatas, mas úteis. Aprendizagem de máquina objetiva “simular” sistemas computacionais com sistemas reais, tentando, da melhor forma possível, analisar e classificar esses dados, seja prevendo resultados, ou achando padrões de agrupamento. O processo é de abstração: obter os dados, organizar, analisar e inferir padrões. Dois os tipos de algoritmos utilizados: o supervisionado e o não supervisionado. O supervisionado (classificação) faz a análise de dados através de modelos matemáticos (lineares, logarítmicos, exponenciais e regressores) com intuito de entender e, posteriormente, prever o comportamento de certo modelo real. Nesse caso, a aprendizagem ocorre por meio de treinamento do classificador com base em experiência prévia. Também deve ser informado o que o algoritmo deve procurar e aprender. O algoritmo não supervisionado (clusterização) não existe conhecimento pré-definido e o agrupamento ocorre por meio da análise de algum critério a fim de encontrar dados similares. Não sendo fornecido previamente o algoritmo deve procurar, o processo de aprendizagem ocorre por meio de regras associativas que são capazes de extrair similaridades dos dados e formar grupos lógicos sem o conhecimento prévio de quais padrões devem ser buscados. Portanto, aprendizagem de máquina resolve o problema de variedade e complexidade de dados com diversas ferramentas, porém, sob condições. É notório que o uso de cada método depende de dois principais fatores: o objetivo do pesquisador e a natureza do dado. Aquele no âmbito de escolher entre o supervisionado e o não supervisionado; este no âmbito de escolher de acordo com questões como tamanho e distribuição dos dados. Assim, um dos requisitos de análise é a quantidade suficiente de agrupamentos (“clusters”) para gerar um determinado padrão amostral. Isto quer dizer que o algoritmo deve permitir explorar e separar dos dados em grupos com características afins, permitindo distinguir os diferentes padrões. Porém, a medida que os dados crescem para análise, é necessário selecionar o algoritmo adequado. Este estudo é financiando pelo programa Pró-Ativa da Pró-Reitoria de Graduação da Universidade Federal de Ouro Preto. O objetivo é, dada uma amostra de dados, analisar a formação de agrupamentos por meio dos algoritmos não supervisionados K-Means, PAM e CLARA em função do tipo de dados e do número de cluster. "],
["github-rbookdown-como-publicar-livros.html", "2 GitHub &amp; Rbookdown - Como publicar livros", " 2 GitHub &amp; Rbookdown - Como publicar livros O GitHub é um sistema de compartilhamento de dados que permite um controle de versão de arquivos. Isso porque ele se baseia no Git, uma ferramenta de VCS (Sistema de Controle de Versão). O fato do GitHub ser uma plataforma OpenSource possibilita que outros usuários colaborem com os trabalhos lá postados em modo público, sugerindo otimizações em novas versões, que, não modificam o arquivo original. Para publicar um Rbookdown em uma página Github são necessários alguns passos Instalando o Git no coputador Como o Git a linguagem que serve como base para o GitHub, primeiramente deve-se baixa-lo no computador. Para isso, acesse o link, escolha o sistema operacional, faça o download e instale o programa normalmente. Criar um repositório no GitHub Primeiramente deve-se criar uma conta na pataforma GitHub. Para isso, deve-se entrar na opção Sign upno site e cadastrar um nome de usuário, email e uma senha. Criado a conta, na página inicial do GitHub, deve-se clicar na opção criar um novo projeto, que irá abrir a pagina Create a new repository que pode ser observado na figura (2.1). Figure 2.1: Janela do repositório Repositórios são análogos de pastas, portanto para cada projeto deve-se criar um novo repositório. Para isso, deve-se definir um nome e, opicionalmente, uma descrição do seu projeto. Antes de criar o repositório marque a opção Initialize this repository with a README. Repositórios em modo público podem ser criadas de forma ilimitada e gratuita, porém, no modo privado requerem uma assinatura mensal. Feito isso, será exibido a página inial do repositório, ilustrado pela figura (2.2). Figure 2.2: Página inicial do repositório A página inicial é composta, por exemplo, de uma barra de configurações gerais do repositório, opções de branch -espécie da subpastas- e opções de criar,deletar e editar arquivos. Por agora, deve-se semente cliar em Clone or Download e copiar o link. Criando um projeto de Rbookdown no R Há diversas formas de criar o livro em linguagem R. Aqui será abordado a utilização de arquivos Rmarkdown Prmeiramente, deve-se abrir o RStudio e sincronizar uma pasta do computador com o repositório no Github. Para isso deve-se clicar na aba file que abrirá a janela New Project. Nesta janela selecionar Version Control e, posteriomente, Git. Feito isso, aparecerá três opções para serem preenchidas, como pode ser observado na figura 3. Figure 2.3: Janela New Project Na aba Repository URL cole o link copiado no passo anterior. Na aba Project directory name dê um nome ao diretório. Na aba Create project as subdirectory of: selecione um destino no computador para a pasta compartilhada com o repositório. Clique em Create a project para finalizar a operação. Configurando o livro para publicação Para que o GitHub gere um livro em um URL são necessários três arquivos essencialmente: Index.Rmd Esse arquivo vem com as informações do livro, como nome do autor e do livro, descrição, e, em alguns casos, um prefácio. Demais arquivos “.Rmd” podem ser criados com as iniciais 01-/02-/03-/…; onde o número representa a ordem que os capítulos serão organizados. _bookdown.yml Sendo o arquivo responsável pelo output, neste arquivo se edita as configurações do documento. Existe uma infinidade de funções que porem customizar o documento exibido que são bem descritas aqui. .nojekyll Esse arquivo é responsável somente por disponibilizar a opção “master branch /docs folder” no GitHub que será útil na hora de se criar a url que ficará disponível o livro. Portanto, nessa etapa deve-se criar os três arquivos na pasta escolhida como diretório no passo 3. O .nojekyll é criado com o comando file.create('.nojekyll') que pode ser digitado no Rstudio. Tanto o **_bookdown.Yml** e o Index.Rmd são arquivos .Rmd, portanto, devem ser cria-los através dos comandos File,New File e R-markdown…; respectivamente. Na janela que será aberta basta dar Ok uma vez que as configurações serão mudadas através dos códigos. O **_bookdown.Yml** deve conter a linha output_dir: &quot;docs&quot;. Já o Index.Rmd deve conter o código, que é dado por: --- title: &quot;Análise de formação de agrupamentos pelos algoritmos não supervisionados K-Means, PAM e Clara&quot; author: &quot;Rodrigo Teotônio&quot; site: bookdown::bookdown_site documentclass: book output: bookdown::gitbook: default #bookdown::pdf_book: default --- Das linhas deste documento, cada uma representa uma configurações do livro. Algumas são parte do código mínimo, portanto não deveram ser alteradas; outras são inforções sobre o projeto que podem e devem ser modificadas. Estas são as configurações title e author, sendo preenchidas, respectivamente, pelo título do livro e o nome do autor. Feito isso, basta clicar na aba “build” no canto superior direito do Rstudio, e selecionar a opção Build Book para visualizar o livro. 5.Exportando os arquivos para o GitHub Criado os arquivos básicos e as demais partes do livro em arquivos .Rmd, agora pode-se exportar os arquivos do diretório do computador para o repositírio no GitHub através do Git. Para isso, abra o Git, e digite os seguintes comandos: git config --global user.nYame &quot;YOUR NAME&quot; // Faça o login com o usuário GitHub; git config --global user.email &quot;YOUR EMAIL ADDRESS&quot; // Faça o login com o email GitHub; cd “O DIRETÓRIO CRIADO EM 2” // Selecione o diretório do Rproject; git init // Seleciona o repositório nesse diretório; git add -A //Adiciona todos os arquivos novos; git commit -m “DESCRIÇÃO DA PRIMEIRA ATUALIZAÇÃO” // Envia para área de teste; git push // Envia para o repositório no git; No GitHub estes arquivos já podem ser vistos na interface principal. Para constuir um Website com o livro, deve-se, na interface do GitHub, clicar na condigurações e, em souce, selecione a opção master branch /docs folder. Feito isso, o GitHub irá gerar o link, que pode ser visualizado em cima da opção. "],
["algoritmos-nao-supervisionados.html", "3 Algoritmos não supervisionados", " 3 Algoritmos não supervisionados Agrupamento, ou “clustering”, é uma técnica para dividir em grupos um conjunto de dados de acordo com alguma similaridade. Essencialmente, três são os métodos de agrupamento de dados por algoritmo não supervisionado para aprendizagem de máquina: K-means, PAM e CLARA. O primeiro, K-means, agrupa (categoriza) os dados por similaridade utilizando o algoritmo Hartigan-Wong (Hartigan and Wong 1979), que calcula a distância Euclidiana entre elementos de acordo com a equação (3.1) Equação 3.1 \\[\\begin{equation} W(c_{k})=\\sum_{X_{1}\\in_{C_{k}}}({x_{i} - u_{k}})^2 \\end{equation}\\] sendo W o grau de similaridade entre pontos; Ck o K-ésimo cluster; Xi pontos para classificação e uK o centroide do cluster Ck, que é calculado pela média das distâncias dos pontos Xi pertencentes a um cluster Ck. O K-means funciona de forma que o usuário entra com os dados e com uma quantidade de clusters, e o algoritmo, primeiramente, determina centroides aleatórios para cada cluster. A partir desses centroides este agrupa os pontos formando então os primeiros clusters. Estes têm seus centroides agora calculados pela média dos pontos agrupados no cluster. A partir desses novos centroides, é feito um reagrupamento dos dados, que é seguido de um recalculo de centroides e outro reagrupamento de dados. Daí em diante o K-means executa esse procedimento um número de vezes de forma que minimize a equação (3.1) entre pontos dentro de um mesmo cluster, e maximize, entre clusters diferentes. Portanto o K-means é um algoritmo muito simples, ao mesmo tempo que extremamente eficiente. Porem existe alguns problemas fruto da forma de sua execução. O primeiro desrespeito ao fato de o número de clusters ser escolhido pelo usuário. Isso porque o K-means tem como principal utilização justamente encontrar padrões em dados, inicialmente, randômicos. Portanto há altas chances do número de clusters sugerido pelo usuário não ser o ideal para seu modelo, afetando drasticamente o resultado final. No R, para tentar minimizar o primeiro problema, tem-se um pacote denominado factorextra (2017). Tal pacote é extremamente útil para visualização de clusters, bem como traz ferramentas que auxiliam na escolha do número ideal de clusters para cada dado que serão discutidas mais à frente. Outro problema está relacionado com o fato do algoritmo escolher pontos iniciais para centroides de forma aleatória. Isso porque o K-means executa o processo de redefinir clusters uma quantidade definida de vezes, sendo por padrão no R dez vezes, mas podendo ser modificado pelo usuário na execução. Assim um determinado dado que tem seus clusters não muito bem definidos pode ser agrupado de forma diferente para cada execução do algoritmo. Uma alternativa a esse problema é oferecida pelo próprio algoritmo que permite o usuário definir o número de vezes que o algoritmo deve rodar escolhendo centroides iniciais aleatórios, onde ele entrega, por fim, o resultado que atenda melhor a equação (3.1). Porém uma solução mais eficiente à esta fragilidade do K-means pode ser superada por outro algoritmo mais robusto, que é menos sensível a pontos aleatórios denominado Partitioning Around Medoids (PAM). O PAM é um algoritmo presente no pacote cluster (2017) e o funcionamento dele é bastante parecido com o funcionamento do K-means, ou seja, ele atribui centros para clusters e agrupa os dados em cada um desses centos de acordo com o grau de similaridade dos pontos. A diferença porem, no PAM é de como ele faz isso. No K-means os centroides iniciais são selecionados de forma aleatória, podendo serem pontos, inclusive, que não fazem parte dos dados, enquanto a similaridade entre cada ponto é dada através da distância Euclidiana, ou seja, o quadrado das distâncias entre os pontos. Já no PAM é calculado a similaridade entre todos os pontos dos dados, escolhendo-se então pontos para servirem de centro para clusters, para que, a partir desses pontos, o restante seja agrupado de acordo com a similaridade entre cada centro. Assim como a similaridade é calculada pela distância de Manhattan (também conhecida por geometria do taxi). Portanto o cálculo da similaridade pelo método PAM obedece a equação (3.2) Equação 3.2 \\[\\begin{equation} W(c_{k})=\\sum_{X_{1}\\in_{C_{k}}}|{X_{1} - X{2}}| \\end{equation}\\] sendo que o grau de similaridade W passa a ser calculado pelo valor absoluto da diferença entre o vetor X1 e X2 de dimensões n, que representam dois pontos pertencentes ao conjunto de dados Ck. Portanto o PAM funciona de forma que o usuário determine os dados a serem agrupados e a quantidade de clusters; e o algoritmo aplica a equação (3.2) para todos os pontos pertencentes aos dados escolhendo os centros, estes denominados no PAM de “medoids” por, diferentemente dos centroides do K-means, terem que pertencer aos dados. Daí cada “medoid” representa o centro de um cluster, portanto todos os outros dados são agrupados em cada um dos clusters de acordo com a similaridade entre estes e aqueles. A partir daí o procedimento é repetido, aplicando-se a equação (3.2) dentro de cada cluster para definir novos “medoids”, e reagrupando todos os dados em função disso; até que se tenha, mas uma vez, uma menor similaridade entre elementos do mesmo cluster e uma maior entre clusters. É notório que o PAM é um algoritmo mais completo que o K-means, o que traz vantagens sob este, uma vez que não é tão sensível a dados muito randômicos. Mas ao mesmo tempo não consegue solucionar o problema de deixar para o usuário a atribuição de número de clusters. Porem essa complexidade do PAM também gera um grande problema: o tempo de execução. Como algoritmo calcula a similaridade de todos os pontos entre si, para dados muito grandes o tempo de execução pode se tornar absurdamente grande, inviabilizando então, a utilização do método. Para contornar esse problema existe um terceiro algorítimo, baseado no PAM, denominado Clustering Large Applications (CLARA). Tal algoritmo funciona selecionando uma amostra dentro do conjunto de dados e aplicando, para esta amostra o algorítimo PAM, sendo que, essa amostra é por padrão de um tamanho de quarenta mais duas vezes a quantidade de clusters. Dentro dessa amostra se elegem os medoids dos quais servem como parâmetro para o agrupamento de todos os outros pontos dos dados. Então os dados são salvos, e o procedimento se repete, escolhendo outra amostra aleatória. Isso acontece cinco vezes, por padrão; porém, para maior confiabilidade do resultado indica-se um número muito maior de testes. O teste de qualidade para cada vez que o procedimento se repetiu obedece a equação (3.3) sendo que o custo de um “medoid” M em relação a todos os pontos do dado D é igual a soma do fator de similaridade W de cada ponto Oi e seu respectivo “medoid” mais próximo, retornado pela função rep; isso dividido ainda pelo número de dados n. Equação 3.3 \\[\\begin{equation} Cost(M,D)=\\frac{\\sum\\limits_{i=1}^{n}W(O_{i},rep{M,O_{i}})}{n} \\end{equation}\\] Assim, por fim, o CLARA aplica a equação (3.3) nos agrupamentos formados a partir de cada amostra aleatória, escolhendo o resultado menor dentre eles. Ou seja, se o algoritmo repetir o procedimento cinco vezes, tendo então, possivelmente, cinco formas de agrupar os mesmos dados diferentes, o CLARA aplica a Cost cinco vezes, assumindo o agrupamento que possui menor valor deste. "],
["metodos-de-visualizacao-do-numero-ideal-de-clusters.html", "4 Métodos de visualização do número ideal de “clusters”", " 4 Métodos de visualização do número ideal de “clusters” Seja qual for o algoritmo a se utilizar, sempre o primeiro passo para agrupar dados passa pela determinação do número de clusters. Infelizmente este passo, como já falado, é feito pelo usuário. Assim, para dar uma melhor visão a este usuário, podendo minimizar o máximo os erros, nessa etapa fazem-se o uso de métodos computacionais que indicam uma quantidade ideal de clusters. Há mais de vinte métodos disponíveis no pacote factoextra(2017) para fazer essa análise, porém dois desses serão descritos, o “Elbow Method”, ou Método do Cotovelo; e a “Average Silhouette” ou Média Sihueta. Como já discutido, um agrupamento ideal em determinado dado é definido por uma distância mínima entre elementos do mesmo cluster, e máxima entre elementos de clusters diferentes. Após o agrupamento de dados, é efetuado o somatório do grau de similaridade W de cada cluster Ck, e para esse somatório é dado o nome de “Total Within-Cluster Sum of Square (WSS)” e traduzido pela equação (4.1). Equação 4.1 \\[\\begin{equation} WSS={\\sum\\limits_{C_{k}=1}^{k}W(C_{k}))} \\end{equation}\\] O primeiro método, denominado “Elbow Method”, se dá justamente na representação do “Total Within-Cluster Sum of Square” para cada número de clusters e plotagem de um gráfico com os respectivos dados. A partir desse gráfico diz-se que o número de clusters ideal é justamente o “cotovelo” dessa curva, ou seja, é o número no qual, a partir dele, a variação dos valores do WSS seja pequena. O outro método de determinar o número de clusters é a “Silhouette”. Enquando o “Elbow” mede o grau de similaridade dos componentes do mesmo cluster, o “Silhouette” é um fator que mede o quanto cada objeto se assemelha ao seu próprio cluster. Para isso, o método computa o algoritmo escolhido de agrupamento (K-means, PAM ou CLARA) com um número de clusters variando de um até dez, por padrão. O “Silhouette” de um determinado ponto do dado é calculado pela equação (4.2) Equação 4.2 \\[\\begin{equation} S(i)=\\frac{b(i)-a(i)}{max[a(i),b(i)]} \\end{equation}\\] na qual a “Silhouette” S de um dado ponto i, pertencente a um cluster K, é calculado de forma que a(i) é a média das distâncias deste ponto com outros pertencentes ao mesmo cluster; e b(i) é a menor distância entre o ponto em questão e outro de um cluster diferente. No denominador, a função max retorna a maior distância entre a(i) e b(i). Daí a equação (4.2) é aplicada em cada ponto do dado agrupado, e é feito uma média do resultado de todos os esses pontos, sendo então essa, a “Average Silhouette” traduzida na equação (4.3) Equação 4.3 \\[\\begin{equation} Cost(M,D)=\\frac{\\sum\\limits_{i\\in_U}S(i)}{n} \\end{equation}\\] sendo que a “Average Silhouette” Smédia de um dado U é o somatório do “Silhouette” S de cada ponto i pertencente a esse dado, dividido pelo número de pontos n. Esse processo é repetido K vezes para cada n número de clusters, e um gráfico é gerado, com a “Average Silhouette” em função de n. Diz-se que o valor ideal de clusters é quando a “Average Silhouette” é máxima. "],
["aplicando-metodos-de-agrupamento-para-diferentes-tipos-de-dados.html", "5 Aplicando métodos de agrupamento para diferentes tipos de dados", " 5 Aplicando métodos de agrupamento para diferentes tipos de dados Por possuírem características diferentes, cada algoritmo consegue obter um resultado mais satisfatório com determinado tipo de dados. Portanto, ao se escolher o algoritmo ideal deve-se levar em conta fatores como tamanho e distribuição dos dados, tempo de execução do algoritmo e precisão do resultado. O primeiro fator analisado diz respeito ao comportamento de cada algoritmo diante de dados com distribuição de dados randômica. Diz-se que uma distribuição é randômica quando todos os pontos de um dado estão muito próximos um do outro. Para isso, utilizou-se uma amostra aleatória, obtida através da função sample do R. Tal amostra foi, primeiramente, agrupada em três “clusters” utilizando o K-means duas vezes distintas, os resultados foram plotados em gráficos (5.1) (5.2) com o auxílio do pacote de visualização de “cluster” factorextra(2017). Figure 5.1: Primeira distribuição randômica pelo K-means Figure 5.2: Segunda distribuição randômica pelo K-means O gráfico (5.1) foi resultado da primeira tentativa de agrupar o dado, e o gráfico (5.2) foi o resultado obtido após reexecutar o algoritmo nas mesmas conduções. Esperava-se os mesmos resultados nas duas tentativas, porém o K-means encontrou padrões diferentes em cada execução. Isso se dá por principalmente dois fatores, o primeiro desrespeito que o K-means utilizar da distância euclidiana para calcular a similaridade. A distância euclidiana por elevar a diferença das coordenadas de cada ponto ao quadrado torna o cálculo da similaridade sensível a pontos muito difusos. O segundo tem a ver com o ponto inicial escolhido pelo algoritmo. O K-means, partindo de amostras aleatórias como centroides, agrupa e reagrupa os dados até encontrar o menor resultado da equação (3.1), porém, para dados randômicos esse resultado pode ser encontrado com diferentes padrões de agrupamento, dependendo justamente do ponto inicial de centroides. Agrupando a mesma amostra randômica, sob as mesmas circunstâncias, porém agora através do PAM obtém-se os gráficos (5.3) (5.4). Figure 5.3: Primeira distribuição randômica pelo PAM Figure 5.4: Segunda distribuição randômica pelo PAM O gráfico (5.3) é a primeira tentativa de agrupamento e o gráfico (5.2) a segunda. Observa-se que ambos os gráficos são idênticos, atendendo então os resultados esperados. Portanto o PAM não apresenta o mesmo problema do K-means. Isso essencialmente porque o PAM utiliza a distância de Manhattan, menos sensível a pontos difusos, por não elevar a diferença ao quadrado; e porque o ponto inicial é calculado. Dado os resultados conclui-se que, para dados randômicos o PAM é uma melhor opção por confiabilidade. Porém, ao comparar os gráficos (5.3) e (5.1) - este feito através do K-means e aquele do PAM - veremos que são idênticos. Isso mostra que o K-means tem potencial para obter resultados tão bons quanto aos do PAM. Mas outro fator que deve ter em consideração ao escolher entre os métodos é o tamanho dos dados. O PAM apesar de mais confiável é limitado não conseguindo processar dados muito grandes. Portando agora foi exportado uma tabela do IBGE contendo o PIB per capita dos municípios brasileiros nos anos de 2010 e 2011. Agrupou-se em os dados em cinco clusters e plotou-se o resultado nos gráficos (5.5) (5.6), sendo o primeiro através do K-means e o segundo através do CLARA. Figure 5.5: Agrupamento pelo K-means Figure 5.6: Agrupamento pelo CLARA Dado os dois gráficos é visível que os dois algoritmos encontraram padrões diferentes de agrupamentos. A ideia desse agrupamento era verificar se há padrões de PIB dentro das cidades de cada estado brasileiro. Fazendo-se então uma tabela gabarito, que mostra quantas cidades de cada estado foi classificada em cada cluster obtém as seguintes tabelas (1) (2). Tabela 1 Regiões 1 2 3 4 5 Centro-Oeste 173 54 222 12 2 Nordeste 1665 28 108 6 0 Norte 368 9 86 2 0 Sudeste 812 202 655 44 9 Sul 210 196 726 11 3 Tabela 2 Regiões 1 2 3 4 5 Centro-Oeste 154 200 38 56 15 Nordeste 58 209 1504 30 6 Norte 49 176 229 9 2 Sudeste 404 634 414 216 54 Sul 513 394 20 205 14 A tabela (1) se refere do agrupamento pelo K-means, enquanto que a tabela (2) se refere pelo CLARA. Dada as duas tabelas dá para se concluir que o agrupamento pelo método CLARA obteve melhor resultado uma vez que o número majoritário de cidades de cada estado ficou concentrado em três clusters, enquanto pelo K-means em dois. Agora refazendo esse agrupamento, porém pelo método PAM, obtém-se o gráfico (5.7). Figure 5.7: Agrupamento pelo PAM Analisando o gráfico (5.7) observa-se que ele contém o mesmo padrão que o gráfico (5.6), ou seja, tanto o agrupamento no PAM e no CLARA obteve o mesmo resultado. Daí conclui-se que o CLARA é a melhor ferramenta para grandes pois consegue manter a precisão do PAM mas com um tempo de execução absurdamente menor. Agora para comparar a eficiência entre o PAM e o K-means em dados pequenos e não randômicos foi utilizado o dado do banco de dados iris do R. Esse banco de dados consistem no comprimento e largura das pétalas e sépalas de cento e cinquenta flores de três espécies diferentes. Fez-se o processo de agrupamento por cada método e o plotado nos gráficos (8) (5.9). Figure 5.8: Agrupamento pelo K-means Figure 5.9: Agrupamento pelo CLARA Seja o gráfico (5.8) pelo K-means e o (5.9) pelo CLARA, ao analisá-los é notório que ambos algoritmos mais uma vez encontraram padrões diferentes. Assim, fazendo uma tabela gabarito, contendo quantas flores de cada espécie foi classificada em cada cluster obtém se as tabelas (3) (4). Tabela 3 Cluster Setosa Versicolor Virginica 1 0 39 14 2 0 11 36 3 50 0 0 Tabela 4 Cluster Setosa Versicolor Virginica 1 50 0 0 2 0 9 36 3 0 41 14 Seja a tabela (3) pelo K-means e a tabela (4) pelo PAM, é visível que os resultados do PAM foram melhores já que classificou um número maior de versicolores no cluster 3. Com isso conclui-se que o K-means, por ser um algoritmo mais simples, consegue classificar dados de forma rápida e eficaz, podendo ser usado para dados grandes ou pequenos, com exceção de dados randômicos, no qual perde confiabilidade. Já o PAM é um algoritmo robusto, e, justamente por isso, é mais preciso que o K-means ao mesmo tempo que demanda mais tempo para rodar, por isso é ideal para dados pequenos sejam eles randômicos ou não. Por fim o CLARA é uma ótima saída para dados grandes, uma vez que consegue, com agilidade, agrupa-los sem perder muito em precisão para o PAM. "]
]
